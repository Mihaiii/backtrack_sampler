{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Backtrack Sampler demo\n",
        "\n",
        "This notebook includes examples for `transformers` (optional) and `llama-cpp-python` (GGUF, offline-friendly).\n",
        "\n",
        "For local runs:\n",
        "- Put `Llama-3.2-1B-Instruct-Q4_K_M.gguf` in the repo root.\n",
        "- Leave `RUN_TRANSFORMERS = False` unless you have `transformers` + weights available locally.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "here = Path.cwd().resolve()\n",
        "repo_root = here if (here / \"backtrack_sampler\").is_dir() else here.parent\n",
        "\n",
        "if str(repo_root) not in sys.path:\n",
        "    sys.path.insert(0, str(repo_root))\n",
        "\n",
        "MODEL_PATH = repo_root / \"Llama-3.2-1B-Instruct-Q4_K_M.gguf\"\n",
        "if not MODEL_PATH.exists():\n",
        "    raise FileNotFoundError(\n",
        "        f\"GGUF not found at {MODEL_PATH}. Download it from:\\n\"\n",
        "        \"https://huggingface.co/unsloth/Llama-3.2-1B-Instruct-GGUF/resolve/main/\"\n",
        "        \"Llama-3.2-1B-Instruct-Q4_K_M.gguf\"\n",
        "    )\n",
        "\n",
        "DEEPSEEK_PATH = repo_root / \"DeepSeek-R1-Distill-Qwen-1.5B-Q4_K_M.gguf\"\n",
        "\n",
        "RUN_TRANSFORMERS = False\n",
        "RUN_DEEPSEEK = DEEPSEEK_PATH.exists()\n",
        "\n",
        "print(\"repo_root:\", repo_root)\n",
        "print(\"MODEL_PATH:\", MODEL_PATH)\n",
        "print(\"RUN_TRANSFORMERS:\", RUN_TRANSFORMERS, \"| RUN_DEEPSEEK:\", RUN_DEEPSEEK)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# If you're running this in a fresh environment (e.g., Colab), you may need:\n",
        "#   pip install backtrack_sampler llama-cpp-python torch\n",
        "#   pip install transformers\n",
        "#\n",
        "# And download the GGUF(s):\n",
        "#   wget https://huggingface.co/unsloth/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_K_M.gguf\n",
        "#   wget https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-Q4_K_M.gguf\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## transformers + AntiSlop Strategy\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "if not RUN_TRANSFORMERS:\n",
        "    print(\"Skipping transformers example (set RUN_TRANSFORMERS=True in the setup cell).\")\n",
        "else:\n",
        "    import time\n",
        "    import torch\n",
        "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "    from backtrack_sampler import BacktrackSampler, AntiSlopStrategy\n",
        "    from backtrack_sampler.provider.transformers_provider import TransformersProvider\n",
        "\n",
        "    model_name = \"unsloth/Llama-3.2-1B-Instruct\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    slops = [\"because\"]\n",
        "    prompt_text = \"Explain why the sky appears blue, in one short paragraph.\"\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt_text}]\n",
        "    prompt = tokenizer.apply_chat_template(\n",
        "        messages, tokenize=False, add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    provider = TransformersProvider(model, tokenizer, device)\n",
        "    strategy = AntiSlopStrategy(provider, slops)\n",
        "    sampler = BacktrackSampler(provider, strategy)\n",
        "\n",
        "    ts = time.time()\n",
        "    token_stream = sampler.generate(\n",
        "        prompt=prompt, max_new_tokens=256, temperature=0.9\n",
        "    )\n",
        "\n",
        "    for token in token_stream:\n",
        "        print(tokenizer.decode(token, skip_special_tokens=True), end=\"\", flush=True)\n",
        "\n",
        "    print(f\"\\nDuration: {time.time() - ts} seconds\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## transformers + Creative Writing Strategy\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "if not RUN_TRANSFORMERS:\n",
        "    print(\"Skipping transformers example (set RUN_TRANSFORMERS=True in the setup cell).\")\n",
        "else:\n",
        "    import time\n",
        "\n",
        "    from backtrack_sampler import BacktrackSampler, CreativeWritingStrategy\n",
        "    from backtrack_sampler.provider.transformers_provider import TransformersProvider\n",
        "\n",
        "    prompt_text = \"Tell me a short tale of a dragon who is afraid of heights.\"\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt_text}]\n",
        "    prompt = tokenizer.apply_chat_template(\n",
        "        messages, tokenize=False, add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    provider = TransformersProvider(model, tokenizer, device)\n",
        "    strategy = CreativeWritingStrategy(\n",
        "        provider, top_p_flat=0.65, top_k_threshold_flat=9, eos_penalty=0.9\n",
        "    )\n",
        "    sampler = BacktrackSampler(provider, strategy)\n",
        "\n",
        "    ts = time.time()\n",
        "    token_stream = sampler.generate(\n",
        "        prompt=prompt,\n",
        "        max_new_tokens=256,\n",
        "        temperature=0.9,\n",
        "        top_k=20,\n",
        "        top_p=0.9,\n",
        "    )\n",
        "\n",
        "    for token in token_stream:\n",
        "        print(tokenizer.decode(token, skip_special_tokens=True), end=\"\", flush=True)\n",
        "\n",
        "    print(f\"\\nDuration: {time.time() - ts} seconds\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## llama.cpp + AntiSlop Strategy\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import time\n",
        "import torch\n",
        "from llama_cpp import Llama, LlamaRAMCache\n",
        "\n",
        "from backtrack_sampler import BacktrackSampler, AntiSlopStrategy\n",
        "from backtrack_sampler.provider.llamacpp_provider import LlamacppProvider\n",
        "\n",
        "llm = Llama(\n",
        "    model_path=str(MODEL_PATH),\n",
        "    chat_format=\"llama-3\",\n",
        "    verbose=False,\n",
        "    n_ctx=2048,\n",
        "    n_batch=1024,\n",
        ")\n",
        "device = torch.device(\"cpu\")\n",
        "cache = LlamaRAMCache(capacity_bytes=100_000_000)\n",
        "\n",
        "slops = [\"because\"]\n",
        "prompt = \"Explain why the sky appears blue, in one short paragraph.\"\n",
        "\n",
        "provider = LlamacppProvider(llm, cache, device)\n",
        "strategy = AntiSlopStrategy(provider, slops)\n",
        "sampler = BacktrackSampler(provider, strategy)\n",
        "\n",
        "ts = time.time()\n",
        "token_stream = sampler.generate(prompt=prompt, max_new_tokens=256, temperature=0.9)\n",
        "\n",
        "for token in token_stream:\n",
        "    print(provider.decode([token]), end=\"\", flush=True)\n",
        "\n",
        "print(f\"\\nDuration: {time.time() - ts} seconds\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## llama.cpp + Creative Writing Strategy\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import time\n",
        "from backtrack_sampler import BacktrackSampler, CreativeWritingStrategy\n",
        "\n",
        "cache = LlamaRAMCache(capacity_bytes=100_000_000)\n",
        "\n",
        "prompt = \"Tell me a short tale of a dragon who is afraid of heights.\"\n",
        "provider = LlamacppProvider(llm, cache, device)\n",
        "strategy = CreativeWritingStrategy(\n",
        "    provider, top_p_flat=0.65, top_k_threshold_flat=9, eos_penalty=0.9\n",
        ")\n",
        "sampler = BacktrackSampler(provider, strategy)\n",
        "\n",
        "ts = time.time()\n",
        "token_stream = sampler.generate(\n",
        "    prompt=prompt,\n",
        "    max_new_tokens=256,\n",
        "    temperature=0.9,\n",
        "    top_k=20,\n",
        "    top_p=0.9,\n",
        ")\n",
        "\n",
        "for token in token_stream:\n",
        "    print(provider.decode([token]), end=\"\", flush=True)\n",
        "\n",
        "print(f\"\\nDuration: {time.time() - ts} seconds\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## llama.cpp + ReplaceStrategy (and ChainStrategy)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "if not RUN_DEEPSEEK:\n",
        "    print(f\"Skipping DeepSeek example (GGUF not found at {DEEPSEEK_PATH}).\")\n",
        "else:\n",
        "    import time\n",
        "    import torch\n",
        "    from llama_cpp import Llama, LlamaRAMCache\n",
        "\n",
        "    from backtrack_sampler import BacktrackSampler, ReplaceStrategy, ChainStrategy\n",
        "    from backtrack_sampler.provider.llamacpp_provider import LlamacppProvider\n",
        "\n",
        "    llm_ds = Llama(\n",
        "        model_path=str(DEEPSEEK_PATH),\n",
        "        verbose=False,\n",
        "        n_ctx=2048 * 2,\n",
        "        n_batch=2048 * 2,\n",
        "    )\n",
        "    device_ds = torch.device(\"cpu\")\n",
        "    cache_ds = LlamaRAMCache(capacity_bytes=10_000_000)\n",
        "\n",
        "    provider_ds = LlamacppProvider(llm_ds, cache_ds, device_ds)\n",
        "    strategy1 = ReplaceStrategy(\n",
        "        provider_ds,\n",
        "        find=[\" So\", \"So\", \"\\nSo\", \"Therefore\", \" Therefore\", \"\\nTherefore\", \"</think>\"],\n",
        "        replace=\" But let me rephrase the request to see if I missed something.\",\n",
        "        max_replacements=4,\n",
        "    )\n",
        "    strategy2 = ReplaceStrategy(\n",
        "        provider_ds,\n",
        "        find=[\n",
        "            \" But\",\n",
        "            \"But\",\n",
        "            \"\\nBut\",\n",
        "            \" Wait\",\n",
        "            \"Wait\",\n",
        "            \"\\nWait\",\n",
        "            \" Alternatively\",\n",
        "            \"Alternatively\",\n",
        "            \"\\nAlternatively\",\n",
        "        ],\n",
        "        replace=\"\\nOkay, so in conclusion\",\n",
        "        skip_tokens=1024,\n",
        "    )\n",
        "    sampler_ds = BacktrackSampler(provider_ds, ChainStrategy([strategy1, strategy2]))\n",
        "\n",
        "    ts = time.time()\n",
        "    token_stream = sampler_ds.generate(\n",
        "        prompt=\"I currently have 2 apples. I ate one yesterday. How many apples do I have now? Think step by step.\",\n",
        "        max_new_tokens=512,\n",
        "    )\n",
        "    for token in token_stream:\n",
        "        print(provider_ds.decode([token]), end=\"\", flush=True)\n",
        "    print(f\"\\nDuration: {time.time() - ts} seconds\")\n"
      ]
    }
  ]
}
